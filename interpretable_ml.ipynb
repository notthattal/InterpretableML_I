{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIPI 590 - XAI | Assignment #3\n",
    "### Interpretable ML\n",
    "#### Author: Tal Erez\n",
    "#### Colab Notebook:\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/notthattal/AdversarialAI_Patch/blob/main/adversarial_patch.ipynb#scrollTo=jv7U4G4FodQf)\n",
    "\n",
    "### How to Run in Colab\n",
    "\n",
    "1. Verify you are running on a GPU. On the top right of the screen click the down arrow in between \"RAM/Disk\" and \"Gemini\"  -> Change Runtime Type -> T4 GPU -> Save\n",
    "2. Ensure Environment Variables are set/Set Environment Variables. On the left-hand side of the screen click the key -> Add new secret -> set the following environment variables:\n",
    "    - CHECKPOINT_PATH: /content/AdversarialAI_Patch/saved_models/\n",
    "    - DATASET_PATH: /content/AdversarialAI_Patch/data/\n",
    "    - IMAGE_PATH: /content/AdversarialAI_Patch/saved_models/images/\n",
    "    - TORCH_HOME: /content/AdversarialAI_Patch/saved_models/\n",
    "3. For each environment variable make sure notebook access is activated (a switch located to the left-hand side of each individual environment variable)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook uses a [Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn/code) database and attempts to compare predicting the likelihood of customer churn using linear regression, logistic regression and a logistic general additive model\n",
    "\n",
    "### Install required dependencies and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove Colab default sample_data if it exists\n",
    "if os.path.exists(\"./sample_data\"):\n",
    "    !rm -r ./sample_data\n",
    "\n",
    "# Clone GitHub files to colab workspace\n",
    "repo_name = \"InterpretableML_I\"\n",
    "\n",
    "# Check if the repo already exists\n",
    "if not os.path.exists(\"/content/\" + repo_name):\n",
    "    git_path = 'https://github.com/notthattal/InterpretableML_I.git'\n",
    "    !git clone \"{git_path}\"\n",
    "else:\n",
    "    print(f\"{repo_name} already exists.\")\n",
    "\n",
    "# Change working directory to location of notebook\n",
    "path_to_notebook = os.path.join(\"/content/\" + repo_name)\n",
    "%cd \"{path_to_notebook}\"\n",
    "%ls\n",
    "\n",
    "#Install the requirements for this package\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygam import LogisticGAM\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataframe\n",
    "1. Creates a dataframe from the CSV\n",
    "2. Cleans the dataframe by dropping all rows with empty strings, whitespace-only strings and NaN values\n",
    "3. Converts all the \"Yes/No\" categorical columns to binary 1 or 0\n",
    "4. One-hot encodes the non-binary categorical columns and drops duplicate one-hot encoded columns (e.g. 'OnlineBackup_No internet service and 'DeviceProtection_No internet service' would just become \"No Internet Service\")\n",
    "5. Remove the Customer ID column from the dataframe as this feature should not affect the outcome\n",
    "6. Convert the boolean columns from the one-hot-encoding to integers and the totalCharges column to a float instead of an object\n",
    "7. Scales the columns containing continuous data\n",
    "8. Assign the feature columns to X and the target column to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv into the dataframe\n",
    "df = pd.read_csv(\"./data/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "# Strip any whitespace from the column (important for cases with invisible spaces)\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Replace all empty strings and whitespace-only strings with NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Drop rows with any NaN or empty string\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert all binary columns to integers False = 0 and True = 1 \n",
    "yes_no_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\", \"Churn\"]\n",
    "for i in yes_no_cols:\n",
    "    df[i] = df[i].map({'No': 0, 'Yes': 1})\n",
    "df[\"gender\"] = df[\"gender\"].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "#One hot encode dataframe\n",
    "df = pd.get_dummies(df, columns=[\"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\", \n",
    "                                 \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \n",
    "                                 \"StreamingMovies\", \"InternetService\", \"Contract\", \n",
    "                                 \"PaymentMethod\"], drop_first=True)\n",
    "\n",
    "#Drop columns that are essentially duplicates of another column in the df\n",
    "df = df.drop(columns=['MultipleLines_No phone service', 'OnlineSecurity_No internet service', 'OnlineBackup_No internet service', 'DeviceProtection_No internet service', 'TechSupport_No internet service', 'StreamingTV_No internet service', 'StreamingMovies_No internet service'])\n",
    "\n",
    "#remove the ID column as it we will not need it for regression\n",
    "cleaned_df = df.drop(columns=['customerID'])\n",
    "\n",
    "#convert the TotalCharges column from string to float\n",
    "cleaned_df['TotalCharges'] = df['TotalCharges'].astype(float)\n",
    "\n",
    "#convert all boolean columns to integer\n",
    "for col in cleaned_df.columns:\n",
    "    if cleaned_df[col].dtype == 'bool':  # Check for boolean dtype\n",
    "        cleaned_df[col] = cleaned_df[col].astype(int)\n",
    "\n",
    "\n",
    "scale_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the columns containing continuous data\n",
    "cleaned_df[scale_cols] = scaler.fit_transform(cleaned_df[scale_cols])\n",
    "\n",
    "y = cleaned_df[\"Churn\"]\n",
    "X = cleaned_df.drop(columns=[\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Linearity\n",
    "According to this [source](https://bookdown.org/rwnahhas/RMPH/mlr-linearity.html), \"For a binary predictor, the linearity assumption is always true – there are two means (the mean outcome at each level of the predictor) and a straight line always perfectly fits two points.\" Thus, for our case in regards to churn, the Linearity assumption has been met.\n",
    "\n",
    "### Assessing Independence\n",
    "\n",
    "Independence of observations for this dataset is assumed due to each row consisting of an individual user and one user's plan shouldn't influence another's plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing MultiCollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "#create the figure\n",
    "plt.figure(figsize=(14, 12)) \n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", annot_kws={\"size\": 8}, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing MultiCollinearity using VIF ([source](https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/))\n",
    "\n",
    "For deciding on a threshold for which to conclude multicollinearity isn't an issue using VIF, there are a number of different sources that recommend a different threshold. After peering through a few different ones, the most common threshold I saw was 0-5 for no multicollinearity, 5-10 for moderate collinearity and 10+ for high multicollinearity. I chose to use a threshold of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store VIF values\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "\n",
    "#calculate the VIF scores and store them in a column in the vif df\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Create the bar chart to visualize VIF\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(vif['Feature'], vif['VIF'], color='skyblue')\n",
    "plt.xlabel('VIF Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Variance Inflation Factors for Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, MonthlyCharges has a very high multicollinearity score. let's visualize it in another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "corr_with_monthly = correlation_matrix['MonthlyCharges'].drop('MonthlyCharges')  # Drop 'MonthlyCharges' correlation with itself\n",
    "\n",
    "# Step 3: Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(corr_with_monthly.index, corr_with_monthly.values, color=['#1f77b4' if x > 0 else '#d62728' for x in corr_with_monthly])\n",
    "\n",
    "# Step 4: Add labels and title\n",
    "plt.xlabel('Correlation with MonthlyCharges')\n",
    "plt.title('Correlation of Features with MonthlyCharges')\n",
    "\n",
    "# Step 5: Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do about the monthly charge problem?\n",
    "This problem is actually two-fold. First, after dropping the monthly charge column and re-evaluating the VIF, I found the total charges and tenure were still above the threshold of 10 we set. This makes sense, as the longer a person stays with the company the higher the total charges would be. In order to meet our threshold, I chose to drop the monthly charges and tenure columns. Monthly charges can be correlated to many other features such as if the customer pays for fiber optics, phone service etc. Once we drop monthly charges and tenure, we still have PhoneService which surpasses our threshold, so I chose to drop all three columns to meet the threshold we decided on earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the monthly charges, tenure columns and PhoneService columns from the training data\n",
    "X = X.drop(columns=[\"MonthlyCharges\", 'tenure', 'PhoneService'])\n",
    "\n",
    "#Recalculate the VIF for the remaining features\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We are under our threshold.\n",
    "\n",
    "\n",
    "### Fit the linear regression model and get residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#fit the model\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#calculate the residuals\n",
    "residuals = y_test - y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Homoscedasticity\n",
    "\n",
    "To assess for homoscedasticity of residuals I used the Breusch–Pagan test. The result below shows that there is overwhelming evidence of heteroscedasticity in the model, which is expected due to the binary nature of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant to the model\n",
    "X_test_with_const = sm.add_constant(X_test)\n",
    "\n",
    "# Run the Breusch-Pagan test\n",
    "test = het_breuschpagan(residuals, X_test_with_const)\n",
    "\n",
    "# Retrieve and report the p-value\n",
    "p_value = test[1]\n",
    "print(f'Breusch-Pagan p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Normality\n",
    "\n",
    "To assess normality I used a Q-Q plot to plot the residuals and, as we can see below, the normality assumption was violated in the case of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot for residuals\n",
    "sm.qqplot(residuals, line='45')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Autocorrelation\n",
    "\n",
    "Autocorrelation isn't expected as the training data doesn't contain any specific ordering or time-dependent features, but to mathematically verify this assumption I chose to use the Durbin-Watson test as recommended by this [source](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r). As we can see, the statistical result is very close to 2 indicating no autocorrelation of the residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and report the results of the durbin_watson test\n",
    "dw_results = durbin_watson(residuals)\n",
    "print(f'Durbin-Watson statistic: {dw_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and report the MSE and R-Squared metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'r-squared: {r_squared}')\n",
    "\n",
    "# Get and print the intercept and coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_\n",
    "})\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively we can tell by the values of the coefficients that this model did not perform well in determining Churn. Having a negative relationship with TotalCharges implies that as the company increases the total cost of the service, the customer is less likely to churn. As much as the company would love that, it doesn't seem very realistic.\n",
    "\n",
    "In terms of model performance, we know that our values can range between 0 and 1 and thus if we calculate the RMSE using the Mean-Squared Error of approximately 0.145 (RMSE = √MSE = √0.145) we can see the model's prediction on average is ≈ 0.381 off from the actual value. Combined with an R-Squared that only explains about 25.8% of the variance in the model, this leads us to the conclusion that the model is underfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "For Logistic Regression, we have already assed our data for linearity, multicollinearity and independence of observations. For meeting the requirement of a large sample size, there isn't much we can do as these are the customer data we have, unless this is only a portion of our customer base. Then, perhaps using a larger dataset that encompasses more of our customers would be acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "#Predict the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Calculate and report the accuracy and f1 scores\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_val = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'f1-score: {f1_val}')\n",
    "\n",
    "# Get and print the intercept and coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_.flatten()\n",
    "})\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model is close to 80% which doesn't seem too bad. However our f1 score still indicates that this model is underfitting the data and needs improvement\n",
    "\n",
    "### Logistic GAM\n",
    "\n",
    "For GAM, again we have already met the assumptions for linearity, multicollinearity, sample size and know that we have violated homoscedasticity. The results below show almost identical performance to Logistic Regression and thus we can reach the same conclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit the GAM to the training set\n",
    "gam = LogisticGAM() \n",
    "gam.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = gam.predict_proba(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "#Calculate and report the accuracy and f1 scores\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_val = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'f1-score: {f1_val}')\n",
    "\n",
    "# Get and print the intercept and coefficients\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_.flatten()\n",
    "})\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "While linear regression was easy to interpret, we can see based on the metrics calculated that the performance of the model was not ideal. This should be expected as the actual data is binary, and linear regression is meant for continuous variables. For both logistic regression and the GAM we saw similar results in terms of accuracy and f1 score, and were able to interpret that both models are underfitting the data. Given that the GAM took longer to run to retrieve the same results as the logistic regression model, if we follow Occam's Razor, we should select logistic regression as the model chosen to predict churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations:\n",
    "\n",
    "https://www.kaggle.com/datasets/blastchar/telco-customer-churn/code\n",
    "\n",
    "https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "\n",
    "https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r\n",
    "\n",
    "https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity#Testing\n",
    "\n",
    "https://bookdown.org/rwnahhas/RMPH/mlr-linearity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
